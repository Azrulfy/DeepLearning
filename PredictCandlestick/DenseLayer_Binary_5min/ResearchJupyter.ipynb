{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Min Binary Options Predictions on FX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries to train deep nerual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.16.2\n",
      "Pandas version: 0.24.2\n",
      "Matplotlib version: 3.0.3\n",
      "Sklearn version: 0.20.3\n",
      "Keras version: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sklearn\n",
    "import keras\n",
    "#import tensorflow\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print('Numpy version: ' + np.__version__)\n",
    "print('Pandas version: ' + pd.__version__)\n",
    "print('Matplotlib version: ' + matplotlib.__version__)\n",
    "print('Sklearn version: ' + sklearn.__version__)\n",
    "print('Keras version: ' + keras.__version__)\n",
    "#print('Tensorflow-gpu version: ' + tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class object to measure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasureTime:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "    def kill(self):\n",
    "        print ('Time elapsed: ' + time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-self.start)))\n",
    "        del self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Notebook_timer = MeasureTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. - Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Convert OHLC data in to wicks and body sizes of candle stick, also type of candle (Bear or Bull)\n",
    "    2. Convert datetime in to weekday , hour and minute.\n",
    "    3. Calculate RSI\n",
    "    4. Calculate MACD\n",
    "    5. Calculate Bolinger Bands\n",
    "    6. Reshape dataframe (Create 10 more columns with last 10 candle data)\n",
    "    7. Create columns with 11th candle type data (Bear or Bull)\n",
    "    8. Delete OHLC and datetime data\n",
    "    9. Convert to numpay array and return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSI function - Relative strength index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_strength(prices, n=14):\n",
    "\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed >= 0].sum()/n\n",
    "    down = -seed[seed < 0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1. + rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i - 1]  # cause the diff is 1 shorter\n",
    "\n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n - 1) + upval)/n\n",
    "        down = (down*(n - 1) + downval)/n\n",
    "\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1. + rs)\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA function - Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, n, type='simple'):\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    if type == 'simple':\n",
    "        weights = np.ones(n)\n",
    "    else:\n",
    "        weights = np.exp(np.linspace(-1., 0., n))\n",
    "\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    a = np.convolve(x, weights, mode='full')[:len(x)]\n",
    "    a[:n] = a[n]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACD function - Moving average covergance divergance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_convergence(x, nslow=26, nfast=12):\n",
    "    \n",
    "    emaslow = moving_average(x, nslow, type='exponential')\n",
    "    emafast = moving_average(x, nfast, type='exponential')\n",
    "    return emaslow, emafast, emafast - emaslow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main feature enginering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureEnginering(MyRawData):\n",
    "\n",
    "    #calculate time series\n",
    "    #MyRawData['WeekDay'] = ((pd.to_datetime(MyRawData['Gmt time'], format='%d.%m.%Y %H:%M:%S.000', utc=True)).dt.dayofweek.astype(dtype=np.int64)/4)\n",
    "    MyRawData['Gmt time'] = pd.to_datetime(MyRawData['Gmt time'], format='%d.%m.%Y %H:%M:%S.000', utc=True)\n",
    "    #MyRawData['Gmt time'] = MyRawData['Gmt time'].dt.datetime\n",
    "    #MyRawData['Gmt time'] = pd.to_datetime(MyRawData['Gmt time'])\n",
    "    MyRawData['WeekDay'] = (MyRawData['Gmt time'].dt.dayofweek.astype(dtype=np.int64)/4)\n",
    "    MyRawData['Hour'] = (MyRawData['Gmt time'].dt.hour.astype(dtype=np.int64)/23)\n",
    "    MyRawData['Minute'] = (MyRawData['Gmt time'].dt.minute.astype(dtype=np.int64)/59)\n",
    "    print('Step 1: Time conversion to Weekday, Hour, Minute')\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    #calculate candle size parameters in pips\n",
    "    MyRawData['BodyPips'] = abs((MyRawData['Open']-MyRawData['Close']) * 10000)\n",
    "    MyRawData['WickUpPips'] = (abs(MyRawData['Open']-MyRawData['High']) * 10000)\n",
    "    MyRawData['WickDownPips'] = (abs(MyRawData['Close']-MyRawData['Low']) * 10000)\n",
    "    #calculate candle type\n",
    "    MyRawData['type'] = 0\n",
    "    MyRawData.loc[MyRawData['Close'] > MyRawData['Open'] , 'type'] = 1\n",
    "    print('Step 2: Calculation Body,Wicks sizes and type of candle')\n",
    "    \n",
    "    MyRawData['EmaSlow'], MyRawData['EmaFast'], MyRawData['MACD'] = moving_average_convergence(MyRawData['Close'].values, nslow=26, nfast=12)                   \n",
    "    print('Step 3: Calculate MACD')\n",
    "    MyRawData['Ema9'] = moving_average(MyRawData['MACD'].values, 9, type='exponential')\n",
    "    print('Step 4: Calculate EMA 9')\n",
    "    MyRawData['RSI'] = relative_strength(MyRawData['Close'].values) \n",
    "    print('Step 5: Calculate RSI')\n",
    "\n",
    "    #create two new DataFrame columns to hold values of upper and lower Bollinger bands\n",
    "    MyRawData=MyRawData[::-1]\n",
    "    MyRawData['Rolling Mean'] = MyRawData['Close'].rolling(window=21).mean()\n",
    "    MyRawData['Bollinger High'] = MyRawData['Close'].rolling(window=21).mean() + (MyRawData['Close'].rolling(window=21).std() * 2)\n",
    "    MyRawData['Bollinger Low'] = MyRawData['Close'].rolling(window=21).mean() - (MyRawData['Close'].rolling(window=21).std() * 2)\n",
    "    MyRawData=MyRawData[::-1]\n",
    "    print('Step 6: Calculate Bolinger Bands')\n",
    "    \n",
    "    #add 10 more history candle sticks\n",
    "    for CandleCount in range(10):\n",
    "        ActualCandleCount = CandleCount + 1\n",
    "        MyRawData['BodyPips' + '_' + str(ActualCandleCount)] = MyRawData['BodyPips'].shift(-ActualCandleCount)\n",
    "        MyRawData['WickUpPips' + '_' + str(ActualCandleCount)] = MyRawData['WickUpPips'].shift(-ActualCandleCount)\n",
    "        MyRawData['WickDownPips' + '_' + str(ActualCandleCount)] = MyRawData['WickDownPips'].shift(-ActualCandleCount)\n",
    "        MyRawData['WeekDay' + '_' + str(ActualCandleCount)] = MyRawData['WeekDay'].shift(-ActualCandleCount)\n",
    "        MyRawData['Hour' + '_' + str(ActualCandleCount)] = MyRawData['Hour'].shift(-ActualCandleCount)\n",
    "        MyRawData['Minute' + '_' + str(ActualCandleCount)] = MyRawData['Minute'].shift(-ActualCandleCount)\n",
    "        MyRawData['type' + '_' + str(ActualCandleCount)] = MyRawData['type'].shift(-ActualCandleCount)\n",
    "        MyRawData['EmaSlow' + '_' + str(ActualCandleCount)] = MyRawData['EmaSlow'].shift(-ActualCandleCount)\n",
    "        MyRawData['EmaFast' + '_' + str(ActualCandleCount)] = MyRawData['EmaFast'].shift(-ActualCandleCount)\n",
    "        MyRawData['MACD' + '_' + str(ActualCandleCount)] = MyRawData['MACD'].shift(-ActualCandleCount)\n",
    "        MyRawData['Ema9' + '_' + str(ActualCandleCount)] = MyRawData['Ema9'].shift(-ActualCandleCount)\n",
    "        MyRawData['RSI' + '_' + str(ActualCandleCount)] = MyRawData['RSI'].shift(-ActualCandleCount)\n",
    "        MyRawData['Rolling Mean' + '_' + str(ActualCandleCount)] = MyRawData['Rolling Mean'].shift(-ActualCandleCount)\n",
    "        MyRawData['Bollinger High' + '_' + str(ActualCandleCount)] = MyRawData['Bollinger High'].shift(-ActualCandleCount)\n",
    "        MyRawData['Bollinger Low' + '_' + str(ActualCandleCount)] = MyRawData['Bollinger Low'].shift(-ActualCandleCount)\n",
    "        print('Step 7: Generating candle backwards [Candle: ' + str(ActualCandleCount) +']')\n",
    "\n",
    "    #populate las column with the output\n",
    "    MyRawData['output'] = 0\n",
    "    MyRawData.loc[MyRawData['Close'] < MyRawData['Close'].shift(5), 'output'] = 1\n",
    "    print('Step 8: Generating prediction column')\n",
    "    \n",
    "\n",
    "    #del MyRawData['Gmt time']\n",
    "    del MyRawData['Open']\n",
    "    del MyRawData['High']\n",
    "    del MyRawData['Low']\n",
    "    del MyRawData['Close']\n",
    "    del MyRawData['Volume']\n",
    "    print('Step 9: Deleting waste columns')\n",
    "\n",
    "    return MyRawData[50:-50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from CSV to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_pd = pd.read_csv(r'D:\\Users\\Mike\\Desktop\\FX data min\\EURUSD_2010_2018.csv',nrows=3000000)\n",
    "#ddf_pd = pd.read_csv('EURUSD_2018.csv')\n",
    "ddf_pd = ddf_pd[::-1]\n",
    "#ddf_pd = ddf_pd.head(400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Time conversion to Weekday, Hour, Minute\n",
      "Step 2: Calculation Body,Wicks sizes and type of candle\n",
      "Step 3: Calculate MACD\n",
      "Step 4: Calculate EMA 9\n",
      "Step 5: Calculate RSI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Calculate Bolinger Bands\n",
      "Step 7: Generating candle backwards [Candle: 1]\n",
      "Step 7: Generating candle backwards [Candle: 2]\n",
      "Step 7: Generating candle backwards [Candle: 3]\n",
      "Step 7: Generating candle backwards [Candle: 4]\n",
      "Step 7: Generating candle backwards [Candle: 5]\n",
      "Step 7: Generating candle backwards [Candle: 6]\n",
      "Step 7: Generating candle backwards [Candle: 7]\n",
      "Step 7: Generating candle backwards [Candle: 8]\n",
      "Step 7: Generating candle backwards [Candle: 9]\n",
      "Step 7: Generating candle backwards [Candle: 10]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cdbf49cf4589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mddf_pd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFeatureEnginering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddf_pd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c908ba7ff7f4>\u001b[0m in \u001b[0;36mFeatureEnginering\u001b[1;34m(MyRawData)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m#populate las column with the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mMyRawData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mMyRawData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMyRawData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mMyRawData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Step 8: Generating prediction column'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;31m# maybe partial set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtake_split_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# if there is only one block/type, still have to take split path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_is_mixed_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5164\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_mixed_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5165\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5167\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5125\u001b[0m         \"\"\"\n\u001b[0;32m   5126\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5127\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5162\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5164\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_mixed_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5165\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mis_mixed_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mis_mixed_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;31m# Warning, consolidation needs to get checked upstairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 1899\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   1900\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   3147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3148\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3149\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3150\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddf_pd=FeatureEnginering(ddf_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf_pd.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data with features applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data by Input, Output & Traning, Validation, Testing data and decoded list of datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ddf_pd.values[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = ddf_pd.values[:, 1:166]\n",
    "X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ddf_pd.values[:, 166:]\n",
    "del ddf_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data by training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X,Y, test_size=0.5)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train, D_val_and_test = train_test_split(D, test_size=0.5)\n",
    "D_val, D_test = train_test_split(D_val_and_test, test_size=0.5)\n",
    "del D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training data: ' + 'X Input shape: ' + str(X_train.shape) + ', ' + 'Y Output shape: ' + str(Y_train.shape) + ', ' + 'datetime shape: ' + str(Y_train.shape))\n",
    "print('Validation data: ' + 'X Input shape: ' + str(X_val.shape) + ', ' + 'Y Output shape: ' + str(Y_val.shape) + ', ' + 'datetime shape: ' + str(Y_train.shape))\n",
    "print('Test data: ' + 'X Input shape: ' + str(X_test.shape) + ', ' + 'Y Output shape: ' + str(Y_test.shape) + ', ' + 'datetime shape: ' + str(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. -  Train Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=165, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, Y_train,batch_size=500, epochs=10,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. -  Get data insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate won and lost trades on data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_win_loss(confidence,predictions,target):\n",
    "    WON = 0\n",
    "    LOST = 0\n",
    "    counter = 0\n",
    "    for prediction_check in predictions:\n",
    "        if prediction_check > confidence: #or prediction_check < 1-confidence:\n",
    "            if np.argmax(predictions[counter]) == target[counter]:\n",
    "                WON = WON + 1\n",
    "            else:\n",
    "                LOST = LOST + 1\n",
    "        counter = counter + 1\n",
    "    return WON,LOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won_trades,lost_trades = calculate_win_loss(0.98,predictions,Y_test)\n",
    "print('Trades won: ' + str(won_trades) + ', Trades lost: ' + str(lost_trades))\n",
    "print('Profitability: ' + str(round((won_trades*100)/(won_trades+lost_trades),2)) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate consequitive trades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consequitive_trades(confidence,predictions,target):\n",
    "    counter = 0\n",
    "    ConsequtiveStats = []\n",
    "    for prediction_check in predictions:\n",
    "        if prediction_check > confidence: # or prediction_check < 1-confidence:\n",
    "            if np.argmax(predictions[counter]) == target[counter]:\n",
    "                ConsequtiveStats.append(1)\n",
    "            else:\n",
    "                ConsequtiveStats.append(0)\n",
    "        counter = counter + 1\n",
    "    z = [(x[0], len(list(x[1]))) for x in itertools.groupby(ConsequtiveStats)]    \n",
    "    MaxLost = 0\n",
    "    MaxWon = 0\n",
    "    for a in z:\n",
    "        if a[0]==0:\n",
    "            if a[1] > MaxLost: MaxLost = a[1]\n",
    "        else:\n",
    "            if a[1] > MaxWon: MaxWon = a[1]     \n",
    "    return MaxWon,MaxLost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_won,max_lost = calculate_consequitive_trades(0.98,predictions,Y_test)\n",
    "print('Max consequitive wins: ' + str(max_won) + ', Max consequitive looses: ' + str(max_lost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate trades per weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_week_trades(confidence,predictions,target,decoded_weekdays):\n",
    "    counter = 0\n",
    "    WeekDayStats = {'Monday':[0,0],'Tuesday':[0,0],'Wendsday':[0,0],'Thursday':[0,0],'Friday':[0,0]}\n",
    "    for prediction_check in predictions:\n",
    "        if prediction_check > confidence: # or prediction_check < 1-confidence:\n",
    "            if np.argmax(predictions[counter]) == target[counter]:\n",
    "                trade_status = 0\n",
    "            else:\n",
    "                trade_status = 1\n",
    "            if decoded_weekdays[counter][1] == 0: WeekDayStats['Monday'][trade_status] = WeekDayStats['Monday'][trade_status] + 1\n",
    "            if decoded_weekdays[counter][1] == 0.25: WeekDayStats['Tuesday'][trade_status] = WeekDayStats['Tuesday'][trade_status] + 1\n",
    "            if decoded_weekdays[counter][1] == 0.5: WeekDayStats['Wendsday'][trade_status] = WeekDayStats['Wendsday'][trade_status] + 1\n",
    "            if decoded_weekdays[counter][1] == 0.75: WeekDayStats['Thursday'][trade_status] = WeekDayStats['Thursday'][trade_status] + 1\n",
    "            if decoded_weekdays[counter][1] == 1: WeekDayStats['Friday'][trade_status] = WeekDayStats['Friday'][trade_status] + 1\n",
    "        counter = counter + 1\n",
    "        \n",
    "    Weekdays = round(len(predictions)/7200,0)    \n",
    "    for key, value in WeekDayStats.items():\n",
    "        value[0] = round(value[0]/Weekdays,0)\n",
    "        value[1] = round(value[1]/Weekdays,0)\n",
    "    \n",
    "    return WeekDayStats,Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day_trades,traded_weeks = calculate_week_trades(0.98,predictions,Y_test,D_test)\n",
    "print('Data gathered from ' + str(traded_weeks) + ' weeks')\n",
    "print('Average trades on Monday =  Won: ' + str(week_day_trades['Monday'][0]) + ', Lost: ' + str(week_day_trades['Monday'][1]))\n",
    "print('Average trades on Tuesday =  Won: ' + str(week_day_trades['Tuesday'][0]) + ', Lost: ' + str(week_day_trades['Tuesday'][1]))\n",
    "print('Average trades on Wendsday =  Won: ' + str(week_day_trades['Wendsday'][0]) + ', Lost: ' + str(week_day_trades['Wendsday'][1]))\n",
    "print('Average trades on Thursday =  Won: ' + str(week_day_trades['Thursday'][0]) + ', Lost: ' + str(week_day_trades['Thursday'][1]))\n",
    "print('Average trades on Friday =  Won: ' + str(week_day_trades['Friday'][0]) + ', Lost: ' + str(week_day_trades['Friday'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate P&L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p_and_l(confidence,predictions,target,trade_return,starting_balance,trade_percent,withdraw_percent):\n",
    "    counter = 0\n",
    "    withdraws = []\n",
    "    balance = []\n",
    "    balance.append(starting_balance)\n",
    "    \n",
    "    for prediction_check in predictions:\n",
    "        if counter % 7200 == 0: \n",
    "            if balance[len(balance)-1] > 10*starting_balance:\n",
    "                withdraws.append(balance[len(balance)-1] *withdraw_percent)\n",
    "                balance[len(balance)-1] = balance[len(balance)-1] * (1-withdraw_percent)\n",
    "            else:\n",
    "                withdraws.append(0)\n",
    "        if prediction_check > confidence: # or prediction_check < 1-confidence:\n",
    "            \n",
    "            next_balance = balance[len(balance)-1]\n",
    "            trade_size = next_balance*trade_percent\n",
    "            next_balance = next_balance - trade_size\n",
    "            \n",
    "            if np.argmax(predictions[counter]) == target[counter]:\n",
    "                next_balance = next_balance + (trade_size*trade_return)\n",
    "            balance.append(round(next_balance,2))\n",
    "            \n",
    "        counter = counter + 1\n",
    "    return balance,withdraws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_loss,weekly_withdraws = calculate_p_and_l(0.98,predictions,Y_test,1.8,50,0.05,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. -  Plot the charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EURUSD - validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "f = plt.figure(figsize=(15,15))\n",
    "\n",
    "ax = f.add_subplot(321)\n",
    "ax.margins(0.1) \n",
    "ax.plot(hist.history['loss'])\n",
    "ax.plot(hist.history['val_loss'])\n",
    "ax.set_title('Model loss')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['Train', 'Val'], loc='upper right')\n",
    "\n",
    "ax2 = f.add_subplot(322)\n",
    "ax2.margins(0.1) \n",
    "ax2.plot(hist.history['acc'])\n",
    "ax2.plot(hist.history['val_acc'])\n",
    "ax2.set_title('Model accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend(['Train', 'Val'], loc='lower right')\n",
    "\n",
    "ax3 = f.add_subplot(323)\n",
    "labels = 'WON', 'LOST'\n",
    "sizes = [won_trades, lost_trades]\n",
    "explode = (0, 0.1)  \n",
    "ax3.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\n",
    "ax3.set_title('Predictions above 98% confidence')\n",
    "\n",
    "ax4 = f.add_subplot(324)\n",
    "ax4.set_title('Consequitive wins and losses in a row')\n",
    "x = ['Won','Lost']\n",
    "ax4.bar(x, [max_won,max_lost])\n",
    "ax4.set_xticks(x, ('Won','Lost'))\n",
    "\n",
    "ax5 = f.add_subplot(325)\n",
    "TradesWonWeekDay = (sum(week_day_trades['Monday']), sum(week_day_trades['Tuesday']),sum(week_day_trades['Wendsday']), sum(week_day_trades['Monday']), sum(week_day_trades['Friday']))\n",
    "TradesLostWeekDay = (week_day_trades['Monday'][1], week_day_trades['Tuesday'][1], week_day_trades['Wendsday'][1], week_day_trades['Thursday'][1], week_day_trades['Friday'][1])\n",
    "p1 = plt.bar(np.arange(5) , TradesWonWeekDay, 0.35 )\n",
    "p2 = plt.bar(np.arange(5), TradesLostWeekDay, 0.35 )\n",
    "ax5.set_ylabel('Trades')\n",
    "ax5.set_title('Trades per Week Day during Weeks: ' + str(traded_weeks))\n",
    "ax5.set_xticks(np.arange(5), ('Monday', 'Tuesday', 'Wendsday', 'Thursday', 'Friday'))\n",
    "ax5.set_xlabel('Week days')\n",
    "ax5.legend((p1[0], p2[0]), ('WON', 'LOST'))\n",
    "\n",
    "ax6 = f.add_subplot(326)\n",
    "ax6.margins(0.1) \n",
    "ax6.plot(profit_loss)\n",
    "ax6.set_title('Model P&L')\n",
    "ax6.set_ylabel('P&L')\n",
    "ax6.set_xlabel('Trade')\n",
    "ax6.legend('Profit', loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. -  Save and load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# load model\n",
    "model = load_model('model3.h5')\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. -  Evaluate model on other asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GBP/USD  1 minute candle data from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddf_pd2 = pd.read_csv('GBPUSD_2018.csv')\n",
    "ddf_pd2=ddf_pd2[::-1]\n",
    "ddf_pd2=ddf_pd2.head(400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features for loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddf_pd2=FeatureEnginering(ddf_pd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice the dataset to get list of date times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = ddf_pd2.values[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice and normalize the dataset to get the input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = ddf_pd2.values[:, 1:166]\n",
    "X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice the dataset to get the output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y = ddf_pd2.values[:, 166:]\n",
    "del ddf_pd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model predictions on GBP/USD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(X, Y)\n",
    "print('Test accuracy:', test_acc)\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the win/loss ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won_trades,lost_trades = calculate_win_loss(0.99,predictions,Y)\n",
    "print('Trades won: ' + str(won_trades) + ', Trades lost: ' + str(lost_trades))\n",
    "print('Profitability: ' + str(round((won_trades*100)/(won_trades+lost_trades),2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the max losses and wins in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_won,max_lost = calculate_consequitive_trades(0.99,predictions,Y)\n",
    "print('Max consequitive wins: ' + str(max_won) + ', Max consequitive looses: ' + str(max_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average trades per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day_trades,traded_weeks = calculate_week_trades(0.99,predictions,Y,D)\n",
    "print('Data gathered from ' + str(traded_weeks) + ' weeks')\n",
    "print('Average trades on Monday =  Won: ' + str(week_day_trades['Monday'][0]) + ', Lost: ' + str(week_day_trades['Monday'][1]))\n",
    "print('Average trades on Tuesday =  Won: ' + str(week_day_trades['Tuesday'][0]) + ', Lost: ' + str(week_day_trades['Tuesday'][1]))\n",
    "print('Average trades on Wendsday =  Won: ' + str(week_day_trades['Wendsday'][0]) + ', Lost: ' + str(week_day_trades['Wendsday'][1]))\n",
    "print('Average trades on Thursday =  Won: ' + str(week_day_trades['Thursday'][0]) + ', Lost: ' + str(week_day_trades['Thursday'][1]))\n",
    "print('Average trades on Friday =  Won: ' + str(week_day_trades['Friday'][0]) + ', Lost: ' + str(week_day_trades['Friday'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the profit and loss ratios togather with weekly withdraws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_loss,weekly_withdraws = calculate_p_and_l(0.99,predictions,Y,1.8,50,0.05,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the charts for GBP/USD simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "f = plt.figure(figsize=(15,15))\n",
    "\n",
    "\n",
    "ax1 = f.add_subplot(321)\n",
    "ax1.set_title('Weekly withdraws')\n",
    "ax1.bar(np.arange(len(weekly_withdraws)),weekly_withdraws)\n",
    "\n",
    "\n",
    "ax3 = f.add_subplot(323)\n",
    "labels = 'WON', 'LOST'\n",
    "sizes = [won_trades, lost_trades]\n",
    "explode = (0, 0.1)  \n",
    "ax3.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\n",
    "ax3.set_title('Predictions above 98% confidence')\n",
    "\n",
    "ax4 = f.add_subplot(324)\n",
    "ax4.set_title('Consequitive wins and losses in a row')\n",
    "x = ['Won','Lost']\n",
    "ax4.bar(x, [max_won,max_lost])\n",
    "ax4.set_xticks(x, ('Won','Lost'))\n",
    "\n",
    "ax5 = f.add_subplot(325)\n",
    "TradesWonWeekDay = (sum(week_day_trades['Monday']), sum(week_day_trades['Tuesday']),sum(week_day_trades['Wendsday']), sum(week_day_trades['Monday']), sum(week_day_trades['Friday']))\n",
    "TradesLostWeekDay = (week_day_trades['Monday'][1], week_day_trades['Tuesday'][1], week_day_trades['Wendsday'][1], week_day_trades['Thursday'][1], week_day_trades['Friday'][1])\n",
    "p1 = plt.bar(np.arange(5) , TradesWonWeekDay, 0.35 )\n",
    "p2 = plt.bar(np.arange(5), TradesLostWeekDay, 0.35 )\n",
    "ax5.set_ylabel('Trades')\n",
    "ax5.set_title('Trades per Week Day during Weeks: ' + str(traded_weeks))\n",
    "ax5.set_xticks(np.arange(5), ('Monday', 'Tuesday', 'Wendsday', 'Thursday', 'Friday'))\n",
    "ax5.set_xlabel('Week days')\n",
    "ax5.legend((p1[0], p2[0]), ('WON', 'LOST'))\n",
    "\n",
    "ax6 = f.add_subplot(326)\n",
    "ax6.margins(0.1) \n",
    "ax6.plot(profit_loss)\n",
    "ax6.set_title('Model P&L')\n",
    "ax6.set_ylabel('P&L')\n",
    "ax6.set_xlabel('Trade')\n",
    "ax6.legend('Profit', loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. - Make live prediction using Aplhafx API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data downloaded from AlphaVantage free API\n",
    "API KEY: ZFO5PFRVOX72B03T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download live data in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_dataset = pd.read_csv('https://www.alphavantage.co/query?function=FX_INTRADAY&from_symbol=EUR&to_symbol=USD&interval=1min&apikey=ZFO5PFRVOX72B03T&datatype=csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_time = current_dataset.values[0:1,0:1][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified funtcion for live feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureEnginering_AplhaVantage(MyRawData, Quite=True):\n",
    "    #calculate time series\n",
    "    #MyRawData['WeekDay'] = ((pd.to_datetime(MyRawData['Gmt time'], format='%d.%m.%Y %H:%M:%S.000', utc=True)).dt.dayofweek.astype(dtype=np.int64)/4)\n",
    "    MyRawData['timestamp'] = pd.to_datetime(MyRawData['timestamp'], format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    #MyRawData['timestamp2'] = MyRawData['timestamp2'].dt.date\n",
    "    #MyRawData['timestamp2'] = pd.to_datetime(MyRawData['timestamp2'])\n",
    "    MyRawData['WeekDay'] = (MyRawData['timestamp'].dt.dayofweek.astype(dtype=np.int64)/4)\n",
    "    MyRawData['Hour'] = (MyRawData['timestamp'].dt.hour.astype(dtype=np.int64)/23)\n",
    "    MyRawData['Minute'] = (MyRawData['timestamp'].dt.minute.astype(dtype=np.int64)/59)\n",
    "    if Quite == False: print('Step 1: Time conversion to Weekday, Hour, Minute')\n",
    "    \n",
    "    #calculate candle size parameters in pips\n",
    "    MyRawData['BodyPips'] = abs((MyRawData['open']-MyRawData['close']) * 10000)\n",
    "    MyRawData['WickUpPips'] = (abs(MyRawData['open']-MyRawData['high']) * 10000)\n",
    "    MyRawData['WickDownPips'] = (abs(MyRawData['close']-MyRawData['low']) * 10000)\n",
    "    #calculate candle type\n",
    "    MyRawData['type'] = 0\n",
    "    MyRawData.loc[MyRawData['close'] > MyRawData['open'] , 'type'] = 1\n",
    "    if Quite == False: print('Step 2: Calculation Body,Wicks sizes and type of candle')\n",
    "    \n",
    "    MyRawData['EmaSlow'], MyRawData['EmaFast'], MyRawData['MACD'] = moving_average_convergence(MyRawData['close'].values, nslow=26, nfast=12)                   \n",
    "    if Quite == False: print('Step 3: Calculate MACD')\n",
    "    MyRawData['Ema9'] = moving_average(MyRawData['MACD'].values, 9, type='exponential')\n",
    "    if Quite == False: print('Step 4: Calculate EMA 9')\n",
    "    MyRawData['RSI'] = relative_strength(MyRawData['close'].values) \n",
    "    if Quite == False: print('Step 5: Calculate RSI')\n",
    "\n",
    "    #create two new DataFrame columns to hold values of upper and lower Bollinger bands\n",
    "    MyRawData=MyRawData[::-1]\n",
    "    MyRawData['Rolling Mean'] = MyRawData['close'].rolling(window=21).mean()\n",
    "    MyRawData['Bollinger High'] = MyRawData['close'].rolling(window=21).mean() + (MyRawData['close'].rolling(window=21).std() * 2)\n",
    "    MyRawData['Bollinger Low'] = MyRawData['close'].rolling(window=21).mean() - (MyRawData['close'].rolling(window=21).std() * 2)\n",
    "    MyRawData=MyRawData[::-1]\n",
    "    if Quite == False: print('Step 6: Calculate Bolinger Bands')\n",
    "    \n",
    "    #add 10 more history candle sticks\n",
    "    for CandleCount in range(10):\n",
    "        ActualCandleCount = CandleCount + 1\n",
    "        MyRawData['BodyPips' + '_' + str(ActualCandleCount)] = MyRawData['BodyPips'].shift(-ActualCandleCount)\n",
    "        MyRawData['WickUpPips' + '_' + str(ActualCandleCount)] = MyRawData['WickUpPips'].shift(-ActualCandleCount)\n",
    "        MyRawData['WickDownPips' + '_' + str(ActualCandleCount)] = MyRawData['WickDownPips'].shift(-ActualCandleCount)\n",
    "        MyRawData['WeekDay' + '_' + str(ActualCandleCount)] = MyRawData['WeekDay'].shift(-ActualCandleCount)\n",
    "        MyRawData['Hour' + '_' + str(ActualCandleCount)] = MyRawData['Hour'].shift(-ActualCandleCount)\n",
    "        MyRawData['Minute' + '_' + str(ActualCandleCount)] = MyRawData['Minute'].shift(-ActualCandleCount)\n",
    "        MyRawData['type' + '_' + str(ActualCandleCount)] = MyRawData['type'].shift(-ActualCandleCount)\n",
    "        MyRawData['EmaSlow' + '_' + str(ActualCandleCount)] = MyRawData['EmaSlow'].shift(-ActualCandleCount)\n",
    "        MyRawData['EmaFast' + '_' + str(ActualCandleCount)] = MyRawData['EmaFast'].shift(-ActualCandleCount)\n",
    "        MyRawData['MACD' + '_' + str(ActualCandleCount)] = MyRawData['MACD'].shift(-ActualCandleCount)\n",
    "        MyRawData['Ema9' + '_' + str(ActualCandleCount)] = MyRawData['Ema9'].shift(-ActualCandleCount)\n",
    "        MyRawData['RSI' + '_' + str(ActualCandleCount)] = MyRawData['RSI'].shift(-ActualCandleCount)\n",
    "        MyRawData['Rolling Mean' + '_' + str(ActualCandleCount)] = MyRawData['Rolling Mean'].shift(-ActualCandleCount)\n",
    "        MyRawData['Bollinger High' + '_' + str(ActualCandleCount)] = MyRawData['Bollinger High'].shift(-ActualCandleCount)\n",
    "        MyRawData['Bollinger Low' + '_' + str(ActualCandleCount)] = MyRawData['Bollinger Low'].shift(-ActualCandleCount)\n",
    "        if Quite == False: print('Step 7: Generating candle backwards [Candle: ' + str(ActualCandleCount) +']')\n",
    "\n",
    "\n",
    "    #del MyRawData['timestamp2']\n",
    "    del MyRawData['timestamp']\n",
    "    del MyRawData['open']\n",
    "    del MyRawData['high']\n",
    "    del MyRawData['low']\n",
    "    del MyRawData['close']\n",
    "    if Quite == False: print('Step 9: Deleting waste columns')\n",
    "\n",
    "    return MyRawData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_dataset=FeatureEnginering_AplhaVantage(current_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show live data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#current_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the first row to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "current_dataset = min_max_scaler.fit_transform(current_dataset)\n",
    "current_predict = current_dataset[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_live = model.predict(current_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.argmax(prediction_live[0]) == 0: Type='RED'\n",
    "if np.argmax(prediction_live[0]) == 0: Type='GREEN'\n",
    "if prediction_live[0][0] > 0.99 or prediction_live[0][0] < 0.01:\n",
    "    if  Type=='RED': Action='PUT'\n",
    "    if  Type=='GREEN': Action='CALL'\n",
    "else:\n",
    "    Action='Wait'\n",
    "print('Prediction time: ' + str(prediction_time) + ', Next candle will be: ' + Type + ' for ' + str(round(prediction_live[0][0],5)) + '%' + ' --> Action: ' + Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction time: 2019-06-24 11:38:00 Next candle will be: GREEN for 0.89005% Action: Wait')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total time required to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Notebook_timer.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reccurent predictions function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reccurent_predict():\n",
    "    current_dataset = pd.read_csv('https://www.alphavantage.co/query?function=FX_INTRADAY&from_symbol=EUR&to_symbol=USD&interval=1min&apikey=ZFO5PFRVOX72B03T&datatype=csv')\n",
    "    prediction_time = current_dataset.values[0:1,0:1][0][0]\n",
    "    current_dataset=FeatureEnginering_AplhaVantage(current_dataset, True)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    current_dataset = min_max_scaler.fit_transform(current_dataset)\n",
    "    current_predict = current_dataset[:1]\n",
    "    prediction_live = model.predict(current_predict)\n",
    "    if np.argmax(prediction_live[0]) == 0: Type='RED'\n",
    "    if np.argmax(prediction_live[0]) == 0: Type='GREEN'\n",
    "    if prediction_live[0][0] > 0.99:\n",
    "        if  Type=='RED': Action='PUT'\n",
    "        if  Type=='GREEN': Action='CALL'\n",
    "    if prediction_live[0][0] < 0.01:\n",
    "        if  Type=='RED': Action='CALL'\n",
    "        if  Type=='GREEN': Action='PUT'\n",
    "    else:\n",
    "        Action='Wait'\n",
    "    print('Prediction time: ' + str(prediction_time) + ', Next candle will be: ' + Type + ' for ' + str(round(prediction_live[0][0],5)) + '%' + ' --> Action: ' + Action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reccurent_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime=time.time()\n",
    "while True:\n",
    "    Reccurent_predict()\n",
    "    time.sleep(60.0 - ((time.time() - starttime) % 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
